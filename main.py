import os
import logging
import numpy as np
import requests
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles

from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
from dotenv import load_dotenv
import json
from datetime import datetime
from typing import Any

# Function to convert numpy types to Python native types for JSON serialization
def convert_numpy_types(obj: Any) -> Any:
    """Convert numpy types to Python native types for JSON serialization"""
    if isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    return obj

# Import orchestratora
from app.api.full_analysis import orchestrator

# Import moduÅ‚u klastrowania semantycznego
from app.services.semantic_clustering import SemanticClusteringService

# Import moduÅ‚u architektury (ModuÅ‚ 3)
from app.services.architecture_generator import ArchitectureGenerator

# Import moduÅ‚u Content Brief Generator (ModuÅ‚ 4)
from app.services.content_brief_generator import ContentBriefGeneratorService

# ========================================
# ENVIRONMENT SETUP
# ========================================
load_dotenv()

# Logger setup - identyczny z orchestratorem
logger = logging.getLogger("main")
# Poziom logowania jest juÅ¼ ustawiony globalnie w app/__init__.py

# Konfiguracja uvicorn logging - wyÅ‚Ä…cz kolorowe logi
logging.getLogger("uvicorn").setLevel(logging.INFO)
logging.getLogger("uvicorn.access").setLevel(logging.WARNING)  # Wycisz access logi
logging.getLogger("uvicorn.error").setLevel(logging.INFO)

# FastAPI app
app = FastAPI(title="SEO Analysis Tool", version="1.0.0")

# Static files
app.mount("/static", StaticFiles(directory="static"), name="static")

# Templates
templates = Jinja2Templates(directory="templates")

class AnalysisRequest(BaseModel):
    keyword: str
    country: str  # Format: "2616|pl"
    use_cache: bool = True  # Nowy parametr do kontroli cache

# ========================================
# ROUTES
# ========================================

@app.get("/")
async def root():
    """Root endpoint - przekierowanie do analizy SEO"""
    return {"message": "SEO Analysis Tool", "version": "1.0.0", "endpoints": ["/seo-analysis"]}

@app.get("/seo-analysis", response_class=HTMLResponse)
async def seo_analysis_page(request: Request):
    """GÅ‚Ã³wna strona z formularzem analizy SEO"""
    return templates.TemplateResponse("seo_analysis.html", {"request": request})

@app.get("/seo-analysis-modular", response_class=HTMLResponse)
async def seo_analysis_modular_page(request: Request):
    """Modularna wersja strony z formularzem analizy SEO"""
    return templates.TemplateResponse("seo_analysis_modular.html", {"request": request})

@app.post("/api/v6/analyze-complete")
async def run_complete_analysis(data: AnalysisRequest):
    """Uruchamia kompletnÄ… analizÄ™ SEO - TYLKO ZAPIS DO BAZY"""
    try:
        # Parsuj country (format: "location_code|language_code")
        location_code, language_code = data.country.split("|")
        location_code = int(location_code)
        
        logger.info(f"ğŸš€ Rozpoczynam analizÄ™: {data.keyword} ({location_code}|{language_code})")
        
        # Uruchom kompletnÄ… analizÄ™ (zapis do bazy)
        result = await orchestrator.run_complete_analysis(
            keyword=data.keyword,
            location_code=location_code,
            language_code=language_code,
            use_cache=data.use_cache
        )
        
        return result
        
    except ValueError:
        raise HTTPException(status_code=400, detail="NieprawidÅ‚owy format kraju")
    except Exception as e:
        logger.exception(f"âŒ BÅ‚Ä…d analizy: {str(e)}")
        raise HTTPException(status_code=500, detail=f"BÅ‚Ä…d analizy: {str(e)}")

@app.get("/api/v6/keyword-data/{keyword}")
async def get_keyword_data(keyword: str, location_code: int, language_code: str):
    """Pobiera dane sÅ‚owa kluczowego z bazy - SEKCJA 1: EXTENDED HEADER"""
    try:
        logger.info(f"ğŸ“Š Pobieranie danych dla: {keyword} ({location_code}|{language_code})")
        
        # Pobierz dane z bazy przez orchestrator
        keyword_data = await orchestrator.get_keyword_header_data(
            keyword=keyword,
            location_code=location_code,
            language_code=language_code
        )
        
        if not keyword_data:
            raise HTTPException(status_code=404, detail=f"Brak danych dla sÅ‚owa: {keyword}")
        
        return {"success": True, "data": keyword_data}
        
    except Exception as e:
        logger.exception(f"âŒ BÅ‚Ä…d pobierania danych: {str(e)}")
        raise HTTPException(status_code=500, detail=f"BÅ‚Ä…d pobierania danych: {str(e)}")

@app.get("/api/v6/countries")
async def get_countries():
    """Pobiera listÄ™ krajÃ³w do wyboru"""
    try:
        countries = await orchestrator.get_available_countries()
        return {"success": True, "countries": countries}
    except Exception as e:
        logger.exception(f"âŒ BÅ‚Ä…d pobierania krajÃ³w: {str(e)}")
        raise HTTPException(status_code=500, detail=f"BÅ‚Ä…d pobierania krajÃ³w: {str(e)}")

@app.get("/api/v6/status")
async def get_orchestrator_status():
    """Status orchestratora"""
    try:
        cache_size = len(orchestrator.cache)
        return {
            "status": "active",
            "cache_size": cache_size,
            "cache_duration_hours": orchestrator.cache_duration.total_seconds() / 3600
        }
    except Exception as e:
        logger.exception(f"âŒ BÅ‚Ä…d statusu: {str(e)}")
        raise HTTPException(status_code=500, detail=f"BÅ‚Ä…d statusu: {str(e)}")

@app.get("/api/v6/ip")
async def get_public_ip():
    """Zwraca publiczny IP widziany przez usÅ‚ugÄ™ (diagnostyka)"""
    try:
        response = requests.get("https://ipinfo.io/json", timeout=5)
        if response.status_code != 200:
            raise HTTPException(
                status_code=502,
                detail=f"BÅ‚Ä…d pobierania IP: HTTP {response.status_code}"
            )
        return {"success": True, "data": response.json()}
    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"âŒ BÅ‚Ä…d pobierania IP: {str(e)}")
        raise HTTPException(status_code=500, detail=f"BÅ‚Ä…d pobierania IP: {str(e)}")

@app.post("/api/v6/clear-cache")
async def clear_cache():
    """CzyÅ›ci cache orchestratora"""
    try:
        cache_size_before = len(orchestrator.cache)
        orchestrator.cache.clear()
        logger.info(f"ğŸ—‘ï¸ Wyczyszczono cache: {cache_size_before} elementÃ³w")
        return {
            "success": True,
            "message": f"Cache wyczyszczony: {cache_size_before} elementÃ³w",
            "cache_size_before": cache_size_before,
            "cache_size_after": 0
        }
    except Exception as e:
        logger.exception(f"âŒ BÅ‚Ä…d czyszczenia cache: {str(e)}")
        raise HTTPException(status_code=500, detail=f"BÅ‚Ä…d czyszczenia cache: {str(e)}")

@app.get("/api/v6/export-data/{keyword}")
async def export_keyword_data(keyword: str, location_code: int, language_code: str, format: str = "json"):
    """
    Eksportuje kompletne dane sÅ‚owa kluczowego ze wszystkich sekcji
    
    Args:
        keyword: SÅ‚owo kluczowe
        location_code: Kod lokalizacji 
        language_code: Kod jÄ™zyka
        format: Format eksportu ('json' lub 'html')
    
    Returns:
        Kompletne dane w wybranym formacie
    """
    try:
        logger.info(f"ğŸ“¦ Eksportowanie danych dla: {keyword} ({location_code}|{language_code}) - format: {format}")
        
        # Pobierz wszystkie dane z bazy przez orchestrator
        complete_data = await orchestrator.get_keyword_header_data(
            keyword=keyword,
            location_code=location_code,
            language_code=language_code
        )
        
        if not complete_data:
            raise HTTPException(status_code=404, detail=f"Brak danych dla sÅ‚owa: {keyword}")
        
        # Dodaj metadane eksportu
        export_metadata = {
            "export_timestamp": datetime.now().isoformat(),
            "export_format": format,
            "keyword": keyword,
            "location_code": location_code,
            "language_code": language_code,
            "data_completeness": {
                "sections_included": [
                    "SEKCJA 1: EXTENDED HEADER",
                    "SEKCJA 2: EXTENDED CORE SEO METRICS", 
                    "SEKCJA 3: COMPLETE INTENT ANALYSIS",
                    "SEKCJA 4: COMPLETE DEMOGRAPHICS",
                    "SEKCJA 5: EXTENDED TRENDS & SEASONALITY",
                    "SEKCJA 6: COMPLETE GEOGRAPHIC DATA",
                    "SEKCJA 7: ENHANCED COMPETITION ANALYSIS",
                    "SEKCJA 8: ENHANCED RELATED KEYWORDS",
                    "SEKCJA 9: ENHANCED SERP ANALYSIS",
                    "SEKCJA 10: ENHANCED AUTOCOMPLETE ANALYSIS"
                ],
                "total_related_keywords": len(complete_data.get("related_keywords", [])),
                "total_serp_items": len(complete_data.get("serp_items", [])),
                "total_autocomplete_suggestions": len(complete_data.get("autocomplete_suggestions", [])),
                "total_historical_records": len(complete_data.get("historical_data", []))
            }
        }
        
        # Przygotuj finalne dane eksportu
        export_data = {
            "metadata": export_metadata,
            "keyword_data": complete_data
        }
        
        if format.lower() == "json":
            return export_data
        elif format.lower() == "html":
            # Generuj HTML z danymi
            html_content = generate_html_export(export_data)
            return HTMLResponse(content=html_content, media_type="text/html")
        else:
            raise HTTPException(status_code=400, detail="NieobsÅ‚ugiwany format. DostÄ™pne: 'json', 'html'")
        
    except Exception as e:
        logger.exception(f"âŒ BÅ‚Ä…d eksportowania danych: {str(e)}")
        raise HTTPException(status_code=500, detail=f"BÅ‚Ä…d eksportowania danych: {str(e)}")

def generate_html_export(export_data: dict) -> str:
    """Generuje HTML z eksportowanymi danymi"""
    metadata = export_data["metadata"]
    data = export_data["keyword_data"]
    
    # Podstawowy HTML bez zagnieÅ¼dÅ¼onych f-stringÃ³w
    html_template = """
    <!DOCTYPE html>
    <html lang="pl">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Eksport danych SEO - {keyword}</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}
            .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; }}
            .header {{ background: #007bff; color: white; padding: 20px; text-align: center; margin: -30px -30px 30px -30px; }}
            .section {{ margin: 30px 0; padding: 20px; border: 1px solid #ddd; border-radius: 5px; }}
            .metric {{ display: inline-block; margin: 10px; padding: 15px; background: #f8f9fa; border-left: 4px solid #007bff; }}
            .metric-label {{ font-size: 0.9em; color: #666; }}
            .metric-value {{ font-size: 1.2em; font-weight: bold; }}
            .json-display {{ background: #f8f9fa; padding: 15px; border-radius: 4px; overflow: auto; max-height: 300px; font-family: monospace; font-size: 12px; }}
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1>ğŸ” Eksport danych SEO</h1>
                <p>SÅ‚owo: {keyword} | Lokalizacja: {location_code} | JÄ™zyk: {language_code}</p>
                <p>Wygenerowano: {timestamp}</p>
            </div>
            
            <div class="section">
                <h2>ğŸ“Š Podstawowe metryki</h2>
                <div class="metric">
                    <div class="metric-label">SÅ‚owo kluczowe</div>
                    <div class="metric-value">{keyword_name}</div>
                </div>
                <div class="metric">
                    <div class="metric-label">Volume wyszukiwaÅ„</div>
                    <div class="metric-value">{search_volume}</div>
                </div>
                <div class="metric">
                    <div class="metric-label">CPC</div>
                    <div class="metric-value">{cpc}</div>
                </div>
                <div class="metric">
                    <div class="metric-label">TrudnoÅ›Ä‡</div>
                    <div class="metric-value">{difficulty}</div>
                </div>
            </div>

            <div class="section">
                <h2>ğŸ“‹ KOMPLETNE DANE JSON</h2>
                <div class="json-display">
{json_data}
                </div>
            </div>
        </div>
    </body>
    </html>
    """
    
    # Przygotuj dane
    def safe_display(value, fallback="Brak danych"):
        if value is None:
            return fallback
        if isinstance(value, (list, dict)) and len(value) == 0:
            return fallback
        if isinstance(value, str) and value.strip() == "":
            return fallback
        return str(value)
    
    def format_number(value, is_currency=False):
        if value is None:
            return "Brak danych"
        try:
            num = float(value)
            if is_currency:
                return f"${num:,.2f}"
            else:
                return f"{num:,.0f}" if num >= 1000 else f"{num}"
        except:
            return str(value)
    
    # Wygeneruj HTML
    formatted_html = html_template.format(
        keyword=metadata['keyword'],
        location_code=metadata['location_code'],
        language_code=metadata['language_code'],
        timestamp=metadata['export_timestamp'][:19].replace('T', ' '),
        keyword_name=safe_display(data.get('keyword_name')),
        search_volume=format_number(data.get('search_volume')),
        cpc=format_number(data.get('cpc'), is_currency=True),
        difficulty=format_number(data.get('keyword_difficulty')),
        json_data=json.dumps(export_data, indent=2, ensure_ascii=False, default=str)
    )
    
    return formatted_html

# ========================================
# MODUÅ KLASTROWANIA SEMANTYCZNEGO (MODUÅ 1)
# ========================================

# Inicjalizacja serwisu klastrowania (wykorzystuje to samo poÅ‚Ä…czenie Supabase co orchestrator)
from app.api.full_analysis import supabase
clustering_service = SemanticClusteringService(supabase)

class ClusteringRequest(BaseModel):
    keyword_id: str  # UUID sÅ‚owa kluczowego

class ArchitectureGenerationRequest(BaseModel):
    semantic_cluster_id: str  # UUID klastra semantycznego z ModuÅ‚u 1
    architecture_type: str = 'silo'  # 'silo' lub 'clusters'
    domain: str = 'example.com'  # Domena docelowa
    save: bool = True  # Czy zapisaÄ‡ do bazy danych
    user_preferences: dict = {}  # Preferencje uÅ¼ytkownika

@app.post("/api/v6/clustering/start")
async def start_semantic_clustering(data: ClusteringRequest):
    """ğŸš€ Uruchamia proces klastrowania semantycznego dla danego sÅ‚owa kluczowego"""
    try:
        logger.info(f"ğŸ”¬ [ENDPOINT] Otrzymano Å¼Ä…danie klastrowania dla keyword_id: {data.keyword_id}")
        logger.info(f"ğŸ“¨ [ENDPOINT] Dane wejÅ›ciowe: {data.dict()}")
        
        # Uruchom proces klastrowania
        logger.info(f"âš¡ [ENDPOINT] Rozpoczynam klastrowanie...")
        result = await clustering_service.process_semantic_clustering(data.keyword_id)
        
        logger.info(f"âœ… [ENDPOINT] Klastrowanie zakoÅ„czone: {result['groups_found']} grup z {result['total_phrases']} fraz")
        logger.info(f"ğŸ“ˆ [ENDPOINT] JakoÅ›Ä‡: {result['quality_score']:.3f}, Koszt: ${result.get('cost_usd', 0.0):.6f}")
        
        response = {
            "success": True,
            "message": f"Klastrowanie zakoÅ„czone pomyÅ›lnie",
            "data": result
        }
        
        logger.info(f"ğŸ“¤ [ENDPOINT] Zwracam odpowiedÅº: {response}")
        return response
        
    except Exception as e:
        logger.exception(f"âŒ [ENDPOINT] BÅ‚Ä…d klastrowania: {str(e)}")
        raise HTTPException(status_code=500, detail=f"BÅ‚Ä…d klastrowania: {str(e)}")

@app.get("/api/v6/clustering/results/{keyword_id}")
async def get_clustering_results(keyword_id: str):
    """ğŸ“Š Pobiera wyniki klastrowania dla danego sÅ‚owa kluczowego"""
    try:
        logger.info(f"ğŸ“Š [ENDPOINT] Otrzymano Å¼Ä…danie wynikÃ³w dla keyword_id: {keyword_id}")
        
        results = await clustering_service.get_clustering_results(keyword_id)
        
        if not results:
            logger.warning(f"âš ï¸ [ENDPOINT] Brak wynikÃ³w klastrowania dla keyword_id: {keyword_id}")
            raise HTTPException(status_code=404, detail="Brak wynikÃ³w klastrowania dla tego sÅ‚owa")
        
        logger.info(f"âœ… [ENDPOINT] Znaleziono wyniki: {len(results['groups'])} grup, {results['total_phrases']} fraz")
        # Bezpieczne logowanie grup
        groups_count = len(results['groups'])
        logger.info(f"ğŸ“Š [ENDPOINT] Liczba grup: {groups_count}")
        
        response = {
            "success": True,
            "data": results
        }
        
        logger.info(f"ğŸ“¤ [ENDPOINT] Zwracam wyniki: {len(results['groups'])} grup")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"âŒ [ENDPOINT] BÅ‚Ä…d pobierania wynikÃ³w: {str(e)}")
        raise HTTPException(status_code=500, detail=f"BÅ‚Ä…d pobierania wynikÃ³w: {str(e)}")

@app.get("/api/v6/clustering/test-ai")
async def test_ai_connection():
    """ğŸ§ª Testuje poÅ‚Ä…czenie z aktywnym AI provider"""
    try:
        result = await clustering_service.test_ai_connection()
        
        if result["success"]:
            logger.info(f"âœ… Test {result.get('provider', 'AI').upper()} pomyÅ›lny: {result['message']}")
        else:
            logger.warning(f"âš ï¸ Test {result.get('provider', 'AI').upper()} nieudany: {result['error']}")
        
        return result
        
    except Exception as e:
        logger.exception(f"âŒ BÅ‚Ä…d testu AI: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "message": "Nieoczekiwany bÅ‚Ä…d podczas testu AI"
        }

@app.get("/api/v6/clustering/test-openai")
async def test_openai_connection():
    """ğŸ§ª Testuje poÅ‚Ä…czenie z OpenAI API (dla kompatybilnoÅ›ci)"""
    try:
        result = await clustering_service.test_openai_connection()
        
        if result["success"]:
            logger.info(f"âœ… Test OpenAI pomyÅ›lny: {result['message']}")
        else:
            logger.warning(f"âš ï¸ Test OpenAI nieudany: {result['error']}")
        
        return result
        
    except Exception as e:
        logger.exception(f"âŒ BÅ‚Ä…d testu OpenAI: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "message": "Nieoczekiwany bÅ‚Ä…d podczas testu OpenAI"
        }

@app.get("/api/v6/clustering/debug/tables")
async def debug_clustering_tables():
    """ğŸ” Debug: Sprawdza czy tabele klastrowania istniejÄ… w bazie"""
    try:
        logger.info(f"ğŸ” [DEBUG] Sprawdzanie tabel klastrowania...")
        
        tables_status = {}
        
        # SprawdÅº semantic_clusters
        try:
            response = await clustering_service._execute_supabase_query_with_retry(
                lambda: clustering_service.supabase.table('semantic_clusters').select('id').limit(1).execute(),
                "check_semantic_clusters"
            )
            tables_status['semantic_clusters'] = {
                'exists': True,
                'sample_count': len(response.data) if response.data else 0
            }
        except Exception as e:
            tables_status['semantic_clusters'] = {
                'exists': False,
                'error': str(e)
            }
        
        # SprawdÅº semantic_groups
        try:
            response = await clustering_service._execute_supabase_query_with_retry(
                lambda: clustering_service.supabase.table('semantic_groups').select('id').limit(1).execute(),
                "check_semantic_groups"
            )
            tables_status['semantic_groups'] = {
                'exists': True,
                'sample_count': len(response.data) if response.data else 0
            }
        except Exception as e:
            tables_status['semantic_groups'] = {
                'exists': False,
                'error': str(e)
            }
        
        # SprawdÅº semantic_group_members (problematyczna tabela)
        try:
            response = await clustering_service._execute_supabase_query_with_retry(
                lambda: clustering_service.supabase.table('semantic_group_members').select('id').limit(1).execute(),
                "check_semantic_group_members"
            )
            tables_status['semantic_group_members'] = {
                'exists': True,
                'sample_count': len(response.data) if response.data else 0
            }
        except Exception as e:
            tables_status['semantic_group_members'] = {
                'exists': False,
                'error': str(e)
            }
        
        logger.info(f"ğŸ” [DEBUG] Status tabel: {tables_status}")
        
        return {
            "success": True,
            "tables": tables_status,
            "recommendation": "JeÅ›li semantic_group_members nie istnieje, uruchom create_semantic_tables.sql w Supabase SQL Editor"
        }
        
    except Exception as e:
        logger.exception(f"âŒ [DEBUG] BÅ‚Ä…d sprawdzania tabel: {str(e)}")
        raise HTTPException(status_code=500, detail=f"BÅ‚Ä…d sprawdzania tabel: {str(e)}")

@app.post("/api/v6/clustering/test-methods")
async def test_clustering_methods(data: ClusteringRequest):
    """ğŸ§ª Testuje i porÃ³wnuje obie metody klastrowania (AI vs Legacy)"""
    try:
        logger.info(f"ğŸ§ª [ENDPOINT] Rozpoczynam test porÃ³wnawczy metod klastrowania dla keyword_id: {data.keyword_id}")
        
        # Uruchom test porÃ³wnawczy
        comparison_result = await clustering_service.test_clustering_methods(data.keyword_id)
        
        if "error" in comparison_result:
            logger.error(f"âŒ [ENDPOINT] BÅ‚Ä…d testu porÃ³wnawczego: {comparison_result['error']}")
            raise HTTPException(status_code=500, detail=comparison_result['error'])
        
        # Loguj wyniki porÃ³wnania
        ai_data = comparison_result.get("ai_clustering", {})
        legacy_data = comparison_result.get("legacy_pipeline", {})
        analysis = comparison_result.get("analysis", {})
        
        logger.info(f"ğŸ¤– [ENDPOINT] AI Clustering: {ai_data.get('num_clusters', 0)} grup, {ai_data.get('noise_ratio', 0):.1%} outliers")
        logger.info(f"ğŸ”§ [ENDPOINT] Legacy Pipeline: {legacy_data.get('num_clusters', 0)} grup, {legacy_data.get('noise_ratio', 0):.1%} outliers")
        logger.info(f"ğŸ† [ENDPOINT] ZwyciÄ™zca: {analysis.get('winner', 'unknown')}")
        
        response = {
            "success": True,
            "message": "Test porÃ³wnawczy zakoÅ„czony pomyÅ›lnie",
            "data": comparison_result
        }
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"âŒ [ENDPOINT] BÅ‚Ä…d testu porÃ³wnawczego: {str(e)}")
        raise HTTPException(status_code=500, detail=f"BÅ‚Ä…d testu porÃ³wnawczego: {str(e)}")

@app.get("/api/v6/clustering/config")
async def get_clustering_config():
    """âš™ï¸ Pobiera aktualnÄ… konfiguracjÄ™ systemu klastrowania"""
    try:
        config = {
            "use_ai_clustering": clustering_service.use_ai_clustering,
            "ai_provider": clustering_service.ai_provider,
            "openai_model": clustering_service.openai_model,
            "claude_model": clustering_service.claude_model,
            "openai_client_available": clustering_service.openai_client is not None,
            "claude_client_available": clustering_service.claude_client is not None,
            "embedding_model": clustering_service.embedding_model,
            "embedding_dimensions": clustering_service.embedding_dimensions,
            "recommended_settings": {
                "USE_AI_CLUSTERING": "true",
                "AI_PROVIDER": "openai",
                "note": "OpenAI zawsze wymagany dla embeddingÃ³w, nawet gdy uÅ¼ywasz Claude do klastrowania"
            }
        }
        
        logger.info(f"âš™ï¸ [ENDPOINT] Zwracam konfiguracjÄ™: AI={config['use_ai_clustering']}, Provider={config['ai_provider']}")
        
        return {
            "success": True,
            "config": config
        }
        
    except Exception as e:
        logger.exception(f"âŒ [ENDPOINT] BÅ‚Ä…d pobierania konfiguracji: {str(e)}")
        raise HTTPException(status_code=500, detail=f"BÅ‚Ä…d pobierania konfiguracji: {str(e)}")

# ========================================
# MODUÅ 3: ARCHITECTURE GENERATOR ENDPOINTS
# ========================================

@app.post("/api/v6/architecture/generate")
async def generate_architecture(data: ArchitectureGenerationRequest):
    """ğŸ—ï¸ GÅÃ“WNY ENDPOINT - Generuje architekturÄ™ strony na podstawie klastra semantycznego"""
    try:
        logger.info(f"ğŸ—ï¸ [ARCHITECTURE] Otrzymano Å¼Ä…danie generowania dla cluster: {data.semantic_cluster_id}")
        logger.info(f"ğŸ—ï¸ [ARCHITECTURE] Parametry: type={data.architecture_type}, domain={data.domain}")
        
        # Pobierz dane klastra semantycznego z bazy danych (uÅ¼ywajÄ…c funkcji z klastrowania)
        cluster_data = await get_semantic_cluster_data_for_architecture(data.semantic_cluster_id)
        
        if not cluster_data['success']:
            logger.error(f"âŒ [ARCHITECTURE] Nie znaleziono klastra: {data.semantic_cluster_id}")
            raise HTTPException(status_code=404, detail=f"Semantic cluster not found: {data.semantic_cluster_id}")
        
        # SprawdÅº czy klaster ma wystarczajÄ…co danych
        if cluster_data['data']['groups_found'] < 2:
            logger.warning(f"âš ï¸ [ARCHITECTURE] Za maÅ‚o grup w klastrze: {cluster_data['data']['groups_found']}")
            raise HTTPException(status_code=400, detail="Klaster musi mieÄ‡ co najmniej 2 grupy semantyczne")
        
        # Inicjalizuj generator architektury
        try:
            architecture_generator = ArchitectureGenerator(
                cluster_data=cluster_data['data'],
                arch_type=data.architecture_type,
                domain=data.domain,
                supabase_client=supabase if data.save else None,
                user_preferences=data.user_preferences
            )
        except ValueError as ve:
            logger.error(f"âŒ [ARCHITECTURE] BÅ‚Ä…d inicjalizacji generatora: {str(ve)}")
            raise HTTPException(status_code=400, detail=f"Generator initialization error: {str(ve)}")
        
        # Generuj architekturÄ™
        if data.save:
            logger.info("ğŸ’¾ [ARCHITECTURE] GenerujÄ™ i zapisujÄ™ architekturÄ™...")
            result = await architecture_generator.generate_and_save(user_id=None)  # TODO: prawdziwy user_id z auth
        else:
            logger.info("ğŸ”„ [ARCHITECTURE] GenerujÄ™ architekturÄ™ (bez zapisu)...")
            result = await architecture_generator.generate()
        
        # Przygotuj odpowiedÅº
        response = {
            "success": True,
            "message": "Architektura wygenerowana pomyÅ›lnie",
            "data": result,
            "saved_to_database": data.save and result.get('database_result', {}).get('success', False),
            "meta": {
                "processing_time": result.get('processing_time', 0),
                "total_pages": result.get('stats', {}).get('total_pages', 0),
                "cross_links_found": result.get('stats', {}).get('cross_links_found', 0),
                "seo_score": result.get('seo_score', 0)
            }
        }
        
        logger.info(f"âœ… [ARCHITECTURE] Architektura wygenerowana: {result.get('stats', {}).get('total_pages', 0)} stron, SEO: {result.get('seo_score', 0)}/100")
        
        # Convert numpy types to Python native types for JSON serialization
        response = convert_numpy_types(response)
        return response
        
    except HTTPException:
        raise  # Re-raise HTTP exceptions
    except Exception as e:
        logger.exception(f"âŒ [ARCHITECTURE] BÅ‚Ä…d generowania architektury: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Architecture generation failed: {str(e)}")

@app.get("/api/v6/architecture/list")
async def list_architectures(semantic_cluster_id: str = None, limit: int = 10, offset: int = 0):
    """ğŸ“‹ Lista wygenerowanych architektur"""
    try:
        logger.info(f"ğŸ“‹ [ARCHITECTURE] Lista architektur: cluster={semantic_cluster_id}, limit={limit}")
        
        # BezpoÅ›rednie zapytanie do Supabase
        query = supabase.table('architectures').select('*')
        
        if semantic_cluster_id:
            query = query.eq('semantic_cluster_id', semantic_cluster_id)
        
        query = query.order('created_at', desc=True).limit(limit).offset(offset)
        
        response = query.execute()
        result = {
            'success': True,
            'data': response.data,
            'count': len(response.data) if response.data else 0
        }
        
        if result['success']:
            logger.info(f"âœ… [ARCHITECTURE] ZwrÃ³cono {result['count']} architektur")
            return {"success": True, "architectures": result['data'], "total": result['count']}
        else:
            logger.error(f"âŒ [ARCHITECTURE] BÅ‚Ä…d: {result.get('error', 'Unknown error')}")
            raise HTTPException(status_code=500, detail=result.get('error', 'Database error'))
            
    except Exception as e:
        logger.error(f"âŒ [ARCHITECTURE] list_architectures error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to list architectures: {str(e)}")

@app.get("/api/v6/architecture/test-connection")
async def test_architecture_module():
    """ğŸ§ª Test poÅ‚Ä…czenia moduÅ‚u architektury"""
    try:
        logger.info("ğŸ§ª [ARCHITECTURE] Test poÅ‚Ä…czenia moduÅ‚u...")
        
        # Test database connection
        test_query = supabase.table('architectures').select('id').limit(1).execute()
        db_status = "connected" if test_query else "error"
        
        # Test LLM availability
        llm_status = []
        if os.getenv("OPENAI_API_KEY"):
            llm_status.append("OpenAI")
        if os.getenv("ANTHROPIC_API_KEY"):
            llm_status.append("Claude")
        
        # Test architecture generator initialization
        try:
            test_cluster_data = {
                'seed_keyword': 'test',
                'groups': [{'name': 'test_group', 'phrases': ['test'], 'embeddings_available': False}]
            }
            test_generator = ArchitectureGenerator(test_cluster_data)
            generator_status = "working"
        except Exception as gen_error:
            generator_status = f"error: {str(gen_error)}"
        
        result = {
            "success": True,
            "module": "architecture_generator",
            "version": "1.0.0",
            "status": {
                "database": db_status,
                "llm_clients": llm_status,
                "generator": generator_status
            },
            "endpoints": [
                "POST /api/v6/architecture/generate",
                "GET /api/v6/architecture/list", 
                "GET /api/v6/architecture/{id}",
                "DELETE /api/v6/architecture/{id}",
                "GET /api/v6/architecture/test-connection"
            ]
        }
        
        logger.info(f"âœ… [ARCHITECTURE] Test moduÅ‚u: database={db_status}, LLM={len(llm_status)}, generator={generator_status}")
        return result
        
    except Exception as e:
        logger.exception(f"âŒ [ARCHITECTURE] BÅ‚Ä…d testu moduÅ‚u: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Module test failed: {str(e)}")

@app.get("/api/v6/architecture/{architecture_id}")
async def get_architecture_details(architecture_id: str):
    """ğŸ” Pobiera szczegÃ³Å‚y konkretnej architektury"""
    try:
        logger.info(f"ğŸ” [ARCHITECTURE] Pobieranie szczegÃ³Å‚Ã³w: {architecture_id}")
        
        # BezpoÅ›rednie zapytanie do Supabase
        response = supabase.table('architectures').select('*').eq('id', architecture_id).execute()
        
        if not response.data:
            raise HTTPException(status_code=404, detail="Architecture not found")
        
        architecture = response.data[0]
        
        # Pobierz powiÄ…zane strony
        pages_response = supabase.table('architecture_pages').select('*').eq('architecture_id', architecture_id).execute()
        
        result = {
            'success': True,
            'data': {
                'architecture': architecture,
                'pages': pages_response.data or []
            }
        }
        
        logger.info(f"âœ… [ARCHITECTURE] SzczegÃ³Å‚y pobrane: {len(result['data']['pages'])} stron")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [ARCHITECTURE] get_architecture_details error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get architecture: {str(e)}")

@app.delete("/api/v6/architecture/{architecture_id}")
async def delete_architecture(architecture_id: str):
    """ğŸ—‘ï¸ Usuwa architekturÄ™"""
    try:
        logger.info(f"ğŸ—‘ï¸ [ARCHITECTURE] Usuwanie architektury: {architecture_id}")
        
        # SprawdÅº czy architektura istnieje
        check_response = supabase.table('architectures').select('id').eq('id', architecture_id).execute()
        
        if not check_response.data:
            raise HTTPException(status_code=404, detail="Architecture not found")
        
        # UsuÅ„ powiÄ…zane strony
        supabase.table('architecture_pages').delete().eq('architecture_id', architecture_id).execute()
        
        # UsuÅ„ architekturÄ™
        delete_response = supabase.table('architectures').delete().eq('id', architecture_id).execute()
        
        logger.info(f"âœ… [ARCHITECTURE] Architektura usuniÄ™ta: {architecture_id}")
        return {"success": True, "message": "Architecture deleted successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [ARCHITECTURE] delete_architecture error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to delete architecture: {str(e)}")

# Helper function do pobierania danych klastra
async def get_semantic_cluster_data_for_architecture(semantic_cluster_id: str):
    """
    Pobiera dane klastra semantycznego z embeddingami dla generatora architektury
    Wykorzystuje ten sam mechanizm co ModuÅ‚ 1
    """
    try:
        logger.info(f"ğŸ“Š [ARCHITECTURE] Pobieranie danych klastra: {semantic_cluster_id}")
        
        # 1. Pobierz gÅ‚Ã³wny klaster
        cluster_query = supabase.table('semantic_clusters').select(
            'id, cluster_name, total_phrases, quality_score, created_at'
        ).eq('id', semantic_cluster_id).execute()
        
        if not cluster_query.data:
            return {"success": False, "error": "Semantic cluster not found"}
        
        cluster = cluster_query.data[0]
        
        # 2. Pobierz wszystkie grupy w klastrze
        groups_query = supabase.table('semantic_groups').select(
            'id, group_label, group_number, phrases_count, avg_similarity_score'
        ).eq('semantic_cluster_id', semantic_cluster_id).order('group_number').execute()
        
        if not groups_query.data:
            return {"success": False, "error": "No groups found in cluster"}
        
        # 3. Dla kaÅ¼dej grupy pobierz czÅ‚onkÃ³w Z EMBEDDINGAMI I VOLUME
        enriched_groups = []
        for group in groups_query.data:
            # Pobierz czÅ‚onkÃ³w grupy z embeddingami
            members_query = supabase.table('semantic_group_members').select(
                'phrase, embedding_vector, similarity_to_centroid, source_table, source_id'
            ).eq('group_id', group['id']).execute()
            if not members_query.data:
                continue
            # WyciÄ…gnij frazy i embeddingi
            phrases = []
            phrases_with_details = []
            embeddings = []
            for member in members_query.data:
                phrase = member['phrase']
                # Pobierz search_volume i cpc z tabeli keywords
                kw_query = supabase.table('keywords').select('search_volume, cpc').eq('keyword', phrase).limit(1).execute()
                if kw_query.data:
                    search_volume = kw_query.data[0].get('search_volume', 0)
                    cpc = kw_query.data[0].get('cpc', 0.0)
                else:
                    search_volume = 0
                    cpc = 0.0
                phrases.append(phrase)
                phrases_with_details.append({
                    'phrase': phrase,
                    'search_volume': search_volume,
                    'cpc': cpc
                })
                if member.get('embedding_vector'):
                    embeddings.append(member['embedding_vector'])
            # Oblicz centroid grupy (Å›rednia embeddingÃ³w)
            group_centroid = None
            if embeddings:
                try:
                    embeddings_array = np.array(embeddings)
                    group_centroid = np.mean(embeddings_array, axis=0).tolist()
                except Exception as e:
                    logger.warning(f"âš ï¸ [ARCHITECTURE] Centroid calculation failed for group {group['group_label']}: {e}")
            enriched_groups.append({
                "group_id": group['id'],
                "name": group['group_label'],
                "phrases": phrases,
                "phrases_with_details": phrases_with_details,
                "phrase_count": len(phrases),
                "similarity_score": group.get('avg_similarity_score', 0.85),
                "group_number": group.get('group_number', 0),
                "embeddings_available": len(embeddings) > 0,
                "group_centroid": group_centroid,  # Dla strategic cross-linking
                "individual_embeddings": embeddings  # Dla advanced analysis
            })
        
        # 4. Extract seed keyword from cluster name
        seed_keyword = cluster.get('cluster_name', '').replace('AI Clustering: ', '').strip()
        if not seed_keyword:
            seed_keyword = "unknown"
        
        result_data = {
            "id": cluster['id'],
            "semantic_cluster_id": cluster['id'],
            "seed_keyword": seed_keyword,
            "cluster_name": cluster.get('cluster_name', ''),
            "total_phrases": cluster.get('total_phrases', 0),
            "quality_score": cluster.get('quality_score', 0.0),
            "groups_found": len(enriched_groups),
            "clusters": enriched_groups,  # For compatibility
            "groups": enriched_groups,    # Preferred naming
            "created_at": cluster.get('created_at'),
            "embeddings_ready": any(g.get('embeddings_available', False) for g in enriched_groups),
            "original_cluster_data": {
                "cluster": cluster,
                "groups_raw": groups_query.data,
                "enriched_groups": enriched_groups
            }
        }
        
        logger.info(f"âœ… [ARCHITECTURE] Dane klastra pobrane: {len(enriched_groups)} grup, embeddings={result_data['embeddings_ready']}")
        return {
            "success": True,
            "data": result_data
        }
        
    except Exception as e:
        logger.error(f"âŒ [ARCHITECTURE] Error fetching semantic cluster data: {str(e)}")
        return {"success": False, "error": f"Data fetch error: {str(e)}"}

# Inicjalizacja serwisu content briefÃ³w
content_brief_service = ContentBriefGeneratorService(supabase)

# Inicjalizacja serwisu content scaffold generator (ModuÅ‚ 5)
from app.services.content_scaffold_generator import ContentScaffoldGenerator
content_scaffold_service = ContentScaffoldGenerator(supabase)

import logging
content_brief_api_logger = logging.getLogger("content_brief_api")
content_scaffold_api_logger = logging.getLogger("content_scaffold_api")

@app.post("/api/content-briefs/generate/{page_id}")
async def generate_content_brief(page_id: str):
    content_brief_api_logger.info(f"[API] Start generate_content_brief page_id={page_id}")
    try:
        result = await content_brief_service.generate_content_brief(page_id)
        content_brief_api_logger.info(f"[API] End generate_content_brief page_id={page_id} result={result}")
        return result
    except Exception as e:
        content_brief_api_logger.error(f"[API] Error generate_content_brief page_id={page_id} error={e}")
        return {"success": False, "error": str(e)}

@app.post("/api/content-briefs/batch/{architecture_id}")
async def generate_all_content_briefs(architecture_id: str):
    content_brief_api_logger.info(f"[API] Start generate_all_content_briefs architecture_id={architecture_id}")
    try:
        result = await content_brief_service.generate_all_briefs_for_architecture(architecture_id)
        content_brief_api_logger.info(f"[API] End generate_all_content_briefs architecture_id={architecture_id} result={result}")
        return result
    except Exception as e:
        content_brief_api_logger.error(f"[API] Error generate_all_content_briefs architecture_id={architecture_id} error={e}")
        return {"success": False, "error": str(e)}

@app.get("/api/content-briefs/{brief_id}")
async def get_content_brief(brief_id: str):
    content_brief_api_logger.info(f"[API] Start get_content_brief brief_id={brief_id}")
    try:
        result = await content_brief_service.get_content_brief(brief_id)
        content_brief_api_logger.info(f"[API] End get_content_brief brief_id={brief_id} result={result}")
        if result:
            return {"success": True, "data": result}
        else:
            return {"success": False, "error": "Nie znaleziono brief"}
    except Exception as e:
        content_brief_api_logger.error(f"[API] Error get_content_brief brief_id={brief_id} error={e}")
        return {"success": False, "error": str(e)}

@app.get("/api/content-briefs/architecture/{architecture_id}/summary")
async def get_architecture_briefs_summary(architecture_id: str):
    content_brief_api_logger.info(f"[API] Start get_architecture_briefs_summary architecture_id={architecture_id}")
    try:
        result = await content_brief_service.get_architecture_briefs_summary(architecture_id)
        content_brief_api_logger.info(f"[API] End get_architecture_briefs_summary architecture_id={architecture_id} result={result}")
        return {"success": True, "data": result}
    except Exception as e:
        content_brief_api_logger.error(f"[API] Error get_architecture_briefs_summary architecture_id={architecture_id} error={e}")
        return {"success": False, "error": str(e)}

# ========================================
# MODUÅ 5: CONTENT SCAFFOLD GENERATOR ENDPOINTS
# ========================================

@app.post("/api/content-briefs/{brief_id}/generate-comprehensive-scaffold")
async def generate_comprehensive_scaffold(brief_id: str):
    """ğŸ§  MODUÅ 5: Generuje ultra-szczegÃ³Å‚owy content scaffold"""
    content_scaffold_api_logger.info(f"[API] Start generate_comprehensive_scaffold brief_id={brief_id}")
    
    try:
        # UÅ¼yj nowej gÅ‚Ã³wnej metody, ktÃ³ra obsÅ‚uguje caÅ‚y flow + persistence
        content_scaffold_api_logger.info(f"[API] Generating complete scaffold with persistence...")
        result = await content_scaffold_service.generate_and_save_scaffold(brief_id)
        
        scaffold_result = result['scaffold_data']
        context_size = result['context_pack_size']
        
        # Przygotuj meta-informacje z result
        meta_info = result.get('metadata', {})
        meta_info.update({
            "context_pack_size": context_size,
            "sections_count": len(scaffold_result.get("content_sections", [])),
            "opportunities_integrated": len(scaffold_result.get("faq_integration_strategy", [])),
            "strategic_links": sum(len(section.get("strategic_linking", [])) for section in scaffold_result.get("content_sections", [])),
            "scaffold_id": result.get('scaffold_id'),
            "database_saved": result.get('database_saved', False),
            "generated_at": datetime.now().isoformat()
        })
        
        content_scaffold_api_logger.info(f"[API] Scaffold generated and saved: {meta_info['sections_count']} sections, {meta_info['strategic_links']} strategic links, ID: {meta_info['scaffold_id']}")
        
        response = {
            "success": True,
            "data": scaffold_result,
            "meta": meta_info
        }
        
        content_scaffold_api_logger.info(f"[API] End generate_comprehensive_scaffold brief_id={brief_id} success=True")
        return response
        
    except Exception as e:
        content_scaffold_api_logger.error(f"[API] Error generate_comprehensive_scaffold brief_id={brief_id} error={e}")
        return {
            "success": False, 
            "error": str(e),
            "meta": {
                "brief_id": brief_id,
                "error_at": datetime.now().isoformat()
            }
        }

@app.get("/api/content-briefs/{brief_id}/scaffold")
async def get_content_scaffold(brief_id: str):
    """ğŸ“‹ Pobiera zapisany content scaffold dla briefu"""
    content_scaffold_api_logger.info(f"[API] Start get_content_scaffold brief_id={brief_id}")
    
    try:
        # Pobierz najnowszy scaffold dla tego briefu
        response = supabase.table('content_scaffolds').select('*').eq('brief_id', brief_id).order('created_at', desc=True).limit(1).execute()
        
        if not response.data:
            content_scaffold_api_logger.warning(f"[API] No scaffold found for brief_id={brief_id}")
            return {
                "success": False,
                "error": "No scaffold found for this brief",
                "meta": {"brief_id": brief_id}
            }
        
        scaffold_data = response.data[0]
        
        content_scaffold_api_logger.info(f"[API] End get_content_scaffold brief_id={brief_id} success=True")
        return {
            "success": True,
            "data": {
                "scaffold_id": scaffold_data.get('id'),
                "scaffold_data": scaffold_data.get('scaffold_data'),
                "meta_info": scaffold_data.get('meta_info'),
                "created_at": scaffold_data.get('created_at')
            }
        }
        
    except Exception as e:
        content_scaffold_api_logger.error(f"[API] Error get_content_scaffold brief_id={brief_id} error={e}")
        return {
            "success": False,
            "error": str(e),
            "meta": {"brief_id": brief_id}
        }

@app.get("/api/content-scaffolds/{scaffold_id}")
async def get_scaffold_by_id(scaffold_id: str):
    """ğŸ“‹ Pobiera content scaffold po ID"""
    content_scaffold_api_logger.info(f"[API] Start get_scaffold_by_id scaffold_id={scaffold_id}")
    
    try:
        scaffold = await content_scaffold_service.get_scaffold_by_id(scaffold_id)
        
        if not scaffold:
            content_scaffold_api_logger.warning(f"[API] No scaffold found for scaffold_id={scaffold_id}")
            return {
                "success": False,
                "error": "Scaffold not found",
                "meta": {"scaffold_id": scaffold_id}
            }
        
        # ğŸ” DEBUG: Check if GEO fields are in scaffold_data from DB
        scaffold_data = scaffold.get('scaffold_data', {})
        if isinstance(scaffold_data, dict) and scaffold_data.get('content_sections'):
            if len(scaffold_data['content_sections']) > 0:
                first_section = scaffold_data['content_sections'][0]
                geo_fields_present = {
                    'ai_answer_target': 'ai_answer_target' in first_section,
                    'structural_format': 'structural_format' in first_section,
                    'information_gain': 'information_gain' in first_section,
                    'trust_signals': 'trust_signals' in first_section,
                    'entity_connections': 'entity_connections' in first_section,
                    'semantic_citation': 'semantic_citation' in first_section
                }
                content_scaffold_api_logger.info(f"ğŸ” [API DEBUG] GEO fields in scaffold from DB: {geo_fields_present}")
        
        content_scaffold_api_logger.info(f"[API] End get_scaffold_by_id scaffold_id={scaffold_id} success=True")
        return {
            "success": True,
            "data": scaffold
        }
        
    except Exception as e:
        content_scaffold_api_logger.error(f"[API] Error get_scaffold_by_id scaffold_id={scaffold_id} error={e}")
        return {
            "success": False,
            "error": str(e),
            "meta": {"scaffold_id": scaffold_id}
        }

@app.get("/api/content-scaffolds/test-connection")
async def test_content_scaffold_connection():
    """ğŸ§ª Test poÅ‚Ä…czenia ModuÅ‚u 5 - Content Scaffold Generator"""
    content_scaffold_api_logger.info("[API] Testing content scaffold generator connection...")
    
    try:
        # Test AI providers connection
        ai_status = {
            "provider": content_scaffold_service.ai_provider,
            "openai": "unknown",
            "claude": "unknown"
        }
        
        try:
            if content_scaffold_service.openai_client:
                test_response = content_scaffold_service.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": "Test"}],
                    max_tokens=5
                )
                ai_status["openai"] = "connected"
            else:
                ai_status["openai"] = "not_configured"
        except Exception as openai_error:
            ai_status["openai"] = f"error: {str(openai_error)}"
        
        try:
            if content_scaffold_service.claude_client:
                ai_status["claude"] = "configured"
            else:
                ai_status["claude"] = "not_configured"
        except Exception as claude_error:
            ai_status["claude"] = f"error: {str(claude_error)}"
        
        # Test database connection
        db_status = "unknown"
        try:
            test_query = supabase.table('content_briefs').select('id').limit(1).execute()
            db_status = "connected"
        except Exception as db_error:
            db_status = f"error: {str(db_error)}"
        
        # Test log directory
        # Dla Railway uÅ¼yj /tmp, lokalnie uÅ¼yj bieÅ¼Ä…cego katalogu
        log_dir = "/tmp" if os.getenv("RAILWAY_ENVIRONMENT") else "."
        log_dir_status = "writable" if os.path.exists(log_dir) or os.access(os.path.dirname(log_dir), os.W_OK) else "not_accessible"
        
        result = {
            "success": True,
            "module": "content_scaffold_generator",
            "version": "2.0.0",
            "status": {
                "ai_providers": ai_status,
                "database": db_status,
                "models": {
                    "openai": content_scaffold_service.openai_model,
                    "claude": content_scaffold_service.claude_model,
                    "gpt5": content_scaffold_service.gpt5_model
                },
                "log_directory": log_dir_status,
                "log_path": log_dir
            },
            "endpoints": [
                "POST /api/content-briefs/{brief_id}/generate-comprehensive-scaffold",
                "GET /api/content-briefs/{brief_id}/scaffold",
                "GET /api/content-scaffolds/test-connection"
            ],
            "capabilities": [
                "Enhanced context pack utilization with concrete examples",
                "Psychology-driven scaffold creation",
                "Strategic linking integration",
                "Content opportunities analysis",
                "FAQ psychology integration", 
                "CTA optimization guidance",
                "Schema markup suggestions",
                "AI interaction logging to disk",
                "Improved prompt engineering with specific instructions",
                "Anti-template approach for personalized scaffolds"
            ]
        }
        
        content_scaffold_api_logger.info(f"[API] Content scaffold test completed: AI={ai_status['provider']}, DB={db_status}")
        return result
        
    except Exception as e:
        content_scaffold_api_logger.error(f"[API] Error testing content scaffold connection: {e}")
        return {
            "success": False,
            "error": str(e),
            "module": "content_scaffold_generator"
        }

@app.get("/api/v6/architecture/{architecture_id}/links")
async def get_architecture_links(architecture_id: str):
    """
    ğŸ”— Endpoint testowy do weryfikacji zapisanych linkÃ³w w architecture_links
    """
    try:
        logger.info(f"ğŸ”— [API] Pobieranie linkÃ³w dla architecture: {architecture_id}")
        
        response = supabase.table('architecture_links').select(
            'id, from_page_id, to_page_id, link_type, anchor_text, priority, source, enabled, '
            'similarity_score, funnel_stage, placement, link_context, '
            'from_page:architecture_pages!architecture_links_from_page_id_fkey(name, url_path), '
            'to_page:architecture_pages!architecture_links_to_page_id_fkey(name, url_path)'
        ).eq('architecture_id', architecture_id).order('priority', desc=True).execute()
        
        if not response.data:
            return {
                "success": True,
                "architecture_id": architecture_id,
                "total_links": 0,
                "links": [],
                "summary": {
                    "hierarchy_links": 0,
                    "bridge_links": 0,
                    "funnel_links": 0
                }
            }
        
        # Grupuj linki wedÅ‚ug typu
        hierarchy_links = [l for l in response.data if l['link_type'] == 'hierarchy']
        bridge_links = [l for l in response.data if l['link_type'] == 'bridge']
        funnel_links = [l for l in response.data if l['link_type'] == 'funnel']
        
        # Formatuj dane dla odpowiedzi
        formatted_links = []
        for link in response.data:
            from_page = link.get('from_page', {})
            to_page = link.get('to_page', {})
            
            formatted_link = {
                "id": link['id'],
                "link_type": link['link_type'],
                "source": link['source'],
                "priority": link['priority'],
                "enabled": link['enabled'],
                "from_page": {
                    "id": link['from_page_id'],
                    "name": from_page.get('name', ''),
                    "url": from_page.get('url_path', '')
                },
                "to_page": {
                    "id": link['to_page_id'],
                    "name": to_page.get('name', ''),
                    "url": to_page.get('url_path', '')
                },
                "anchor_text": link.get('anchor_text', ''),
                "placement": link.get('placement', ''),
                "context": link.get('link_context', ''),
                "similarity_score": link.get('similarity_score'),
                "funnel_stage": link.get('funnel_stage', '')
            }
            formatted_links.append(formatted_link)
        
        logger.info(f"âœ… [API] Znaleziono {len(response.data)} linkÃ³w")
        
        return {
            "success": True,
            "architecture_id": architecture_id,
            "total_links": len(response.data),
            "links": formatted_links,
            "summary": {
                "hierarchy_links": len(hierarchy_links),
                "bridge_links": len(bridge_links),
                "funnel_links": len(funnel_links)
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ [API] BÅ‚Ä…d pobierania architecture links: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/debug/scaffold-logs")
async def get_scaffold_logs(limit: int = 5):
    """ğŸ” DEBUG: Pobiera ostatnie logi generowania scaffoldÃ³w z /tmp"""
    try:
        import glob
        log_dir = "/tmp" if os.getenv("RAILWAY_ENVIRONMENT") else "."
        log_pattern = os.path.join(log_dir, "scaffold_generation_*.txt")
        log_files = sorted(glob.glob(log_pattern), key=os.path.getmtime, reverse=True)[:limit]
        
        logs_data = []
        for log_file in log_files:
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    logs_data.append({
                        "filename": os.path.basename(log_file),
                        "size": len(content),
                        "preview": content[:2000] + "..." if len(content) > 2000 else content,
                        "full_content": content
                    })
            except Exception as e:
                logs_data.append({
                    "filename": os.path.basename(log_file),
                    "error": str(e)
                })
        
        return {
            "success": True,
            "log_dir": log_dir,
            "total_logs": len(log_files),
            "logs": logs_data
        }
    except Exception as e:
        logger.error(f"âŒ [DEBUG] Error reading logs: {e}")
        return {"success": False, "error": str(e)}

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))  # Railway przekaÅ¼e PORT
    uvicorn.run(app, host="0.0.0.0", port=port)
